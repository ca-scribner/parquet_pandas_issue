{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Attempt to recreate an issue with reading large Pandas DataFrames from parquet files.  Found the issue where reading some small parquet files to dataframe (~25MB at rest, ~250MB while in a dataframe) led to very high memory usage (>11GB) during intake.  Original problem was on a non-internet connected machine with non-public data, so trying to recreate it.. \n",
    "\n",
    "Can't recreate on linux with \n",
    "* pandas==1.0.3\n",
    "* pyarrow==0.16.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_POOL = string.ascii_letters + string.punctuation + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(n_rows, n_int_cols=0, n_float_cols=0, str_cols=None):\n",
    "    data = {}\n",
    "    for i in range(n_int_cols):\n",
    "        col_name = f\"int_{i}\"\n",
    "        this_data = np.random.randint(0, 65000, size=n_rows)\n",
    "        data[col_name] = this_data\n",
    "\n",
    "    for i in range(n_float_cols):\n",
    "        col_name = f\"float_{i}\"\n",
    "        this_data = np.random.rand(n_rows)\n",
    "        data[col_name] = this_data\n",
    "    \n",
    "    if str_cols is not None:\n",
    "        for i, width in enumerate(str_cols):\n",
    "            col_name = f\"str_{i}\"\n",
    "            this_data = [random_string(width) for i in range(n_rows)]\n",
    "            this_data = np.array(this_data)\n",
    "            data[col_name] = this_data\n",
    "            \n",
    "    return data\n",
    "\n",
    "def random_string(length, letter_pool=LETTER_POOL):\n",
    "    return ''.join(random.choices(letter_pool, k=length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 100000\n",
    "n_int_cols = 5\n",
    "n_float_cols = 5\n",
    "str_cols = [300, 300, 300]\n",
    "df = pd.DataFrame(create_data(n_rows, n_int_cols, n_float_cols, str_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file size = 1876.36MB\n"
     ]
    }
   ],
   "source": [
    "n_dfs = 20\n",
    "test_df = pd.concat([df for i in range(n_dfs)])\n",
    "fname = f'df_{n_dfs}.parquet'\n",
    "test_df.to_parquet(fname)\n",
    "print(f\"Output file size = {os.path.getsize(fname) / 1000000:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4625.16 MiB, increment: 2815.89 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "df_in = pd.read_parquet(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems way less than I was experiencing elsewhere..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parquet_pandas_issue",
   "language": "python",
   "name": "parquet_pandas_issue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
